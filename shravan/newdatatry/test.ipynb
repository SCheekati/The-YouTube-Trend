{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv('C:\\\\Users\\\\cheek\\\\ML-7641-Team14\\\\dataset\\output\\\\dataset_mar_23\\\\new_train.csv')\n",
    "valdf = pd.read_csv('C:\\\\Users\\\\cheek\\\\ML-7641-Team14\\\\dataset\\output\\\\dataset_mar_23\\\\new_val.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate rows check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>cate_0</th>\n",
       "      <th>cate_1</th>\n",
       "      <th>cate_2</th>\n",
       "      <th>cate_3</th>\n",
       "      <th>...</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>views per day</th>\n",
       "      <th>likes per day</th>\n",
       "      <th>dislikes per day</th>\n",
       "      <th>comments per day</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [video_id, title, publishedAt, channelId, channelTitle, categoryId, cate_0, cate_1, cate_2, cate_3, cate_4, num_sub, trending_date, tags, views per day, likes per day, dislikes per day, comments per day, thumbnail_link, rating, description, trending]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows = traindf[traindf.duplicated(['video_id'], keep=False)]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>cate_0</th>\n",
       "      <th>cate_1</th>\n",
       "      <th>cate_2</th>\n",
       "      <th>cate_3</th>\n",
       "      <th>...</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>views per day</th>\n",
       "      <th>likes per day</th>\n",
       "      <th>dislikes per day</th>\n",
       "      <th>comments per day</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [video_id, title, publishedAt, channelId, channelTitle, categoryId, cate_0, cate_1, cate_2, cate_3, cate_4, num_sub, trending_date, tags, views per day, likes per day, dislikes per day, comments per day, thumbnail_link, rating, description, trending]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows = valdf[valdf.duplicated(['video_id'], keep=False)]\n",
    "duplicate_rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Columns\n",
    "TDIL Channel title and title are different things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.drop(['channelId','publishedAt', 'trending_date', 'views per day', 'likes per day', 'dislikes per day', 'comments per day', 'thumbnail_link', 'rating','video_id'], axis=1, inplace=True)\n",
    "valdf.drop(['channelId','publishedAt', 'trending_date', 'views per day', 'likes per day', 'dislikes per day', 'comments per day', 'thumbnail_link', 'rating','video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['lang'] = traindf['title'] + ' ' + traindf['tags'] + ' ' + traindf['description'] + ' ' + traindf['channelTitle']\n",
    "valdf['lang'] = valdf['title'] + ' ' + valdf['tags'] + ' ' + valdf['description'] + ' ' + valdf['channelTitle']\n",
    "traindf.drop(['title', 'tags', 'description', 'channelTitle'], axis=1, inplace=True)\n",
    "valdf.drop(['title', 'tags', 'description', 'channelTitle'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   categoryId  cate_0  cate_1  cate_2  cate_3  cate_4   num_sub  trending  \\\n",
      "0          24       0       0       0       1       1    754000         1   \n",
      "1          17       1       0       0       0       1    314000         1   \n",
      "2          22       0       1       1       0       1   1290000         1   \n",
      "3          22       0       1       1       0       1  10000000         1   \n",
      "4          26       0       1       0       1       1    145000         0   \n",
      "\n",
      "                                                lang  \n",
      "0  Power Book II: Ghost | Official Trailer | Seas...  \n",
      "1  Julian Newman WENT OFF Against Tristan Jass an...  \n",
      "2  GET READY WITH US TO GO TO LES DO MAKEUP'S AND...  \n",
      "3  Binging with Babish: Tater Tots from Breaking ...  \n",
      "4  My Friend sister Birthday Cake doll cake kaise...  \n",
      "   categoryId  cate_0  cate_1  cate_2  cate_3  cate_4   num_sub  trending  \\\n",
      "0           2       0       1       0       0       0    511000         1   \n",
      "1          26       0       1       0       1       1     24400         0   \n",
      "2          10       0       1       0       1       0  26700000         1   \n",
      "3          28       0       0       1       1       1  10700000         1   \n",
      "4          28       0       0       1       1       1   2940000         1   \n",
      "\n",
      "                                                lang  \n",
      "0  What's Next For The Boat That Was Sunk At The ...  \n",
      "1  Tapa Kimchi Rice | Home Foodie Cooking Show #M...  \n",
      "2  TWICE READY TO BE Opening Trailer JYP Entertai...  \n",
      "3  Primitive Technology: Iron Bacteria Cement (no...  \n",
      "4  EGG DROP - I Did A Thing vs William Osman lase...  \n"
     ]
    }
   ],
   "source": [
    "print(traindf.head())\n",
    "print(valdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.drop('categoryId', axis=1, inplace=True)\n",
    "valdf.drop('categoryId', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Cleaning, scaling, normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognormalizesubs(df):\n",
    "    df['log_subs'] = np.log1p(df['num_sub'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognormalizesubs(traindf)\n",
    "lognormalizesubs(valdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = traindf['lang'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cate_0  cate_1  cate_2  cate_3  cate_4  num_sub  trending lang  \\\n",
      "27         0       0       1       1       1   161000         0  NaN   \n",
      "57         1       1       1       1       0   577000         0  NaN   \n",
      "68         1       1       0       1       1   143000         0  NaN   \n",
      "90         0       0       1       1       1   327000         0  NaN   \n",
      "132        0       1       0       0       0  2370000         0  NaN   \n",
      "...      ...     ...     ...     ...     ...      ...       ...  ...   \n",
      "4186       0       0       1       0       1  4720000         1  NaN   \n",
      "4226       1       1       1       0       1   219000         0  NaN   \n",
      "4253       0       0       0       1       1    51700         0  NaN   \n",
      "4281       0       1       0       1       1    79100         0  NaN   \n",
      "4314       0       0       1       0       1    19700         0  NaN   \n",
      "\n",
      "       log_subs  \n",
      "27    11.989166  \n",
      "57    13.265599  \n",
      "68    11.870607  \n",
      "90    12.697719  \n",
      "132   14.678401  \n",
      "...         ...  \n",
      "4186  15.367320  \n",
      "4226  12.296832  \n",
      "4253  10.853232  \n",
      "4281  11.278481  \n",
      "4314   9.888425  \n",
      "\n",
      "[240 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(traindf[traindf['lang'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.dropna(subset=['lang'], inplace=True)\n",
    "valdf.dropna(subset=['lang'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.reset_index(drop=True)\n",
    "valdf = valdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human elevator [none] #parkour Oliver Nordin\n"
     ]
    }
   ],
   "source": [
    "smallest_lang_value = min(traindf['lang'], key=lambda x: len(x))\n",
    "print(smallest_lang_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append('C:/Users/cheek/ML-7641-Team14/shravan/nltk_data')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5510\n"
     ]
    }
   ],
   "source": [
    "max_length = traindf['lang'].apply(len).max()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "323000\n",
      "0\n",
      "Robot 3.0 Full Movie HD | Rajnikant  | Katrina Kaif | Shankar | 2023 | Full  Sci-Fi Movie in Hindi | 3.0 full movie|3.0 full movie in tamil|2.0 full movie|3.0 full movie in hindi|robot 3.0 movie|3.0 in 2.0 movie|3.0 rajinikanth|3.0 trailer tamil|salman in 3.0 movie|2.0 movie|3.0 trailer|robot 3.0 full movie|robot 3.0 trailer full movie|robot 3.0 movies|full movie 2.0|3.0 rajinikanth tamil|3.0 rajinikanth hindi|2.0 full movie in hindi|robot 3.0|full movie|robot 3.0 full hd movie|rajnikant new movie|new south movie 2023|robot #robot3  #AkshayKumar #ShankarRobot 3.0 Full Movie HD | Rajnikant  | Katrina Kaif | Shankar | 2023 | Full  Sci-Fi Movie in Hindi |After a decade of research, scientist Vaseegaran creates a sophisticated android humanoid robot with the help of his assistants, Siva and Ravi, in order to commission it into the Indian Army. He introduces the robot, named Chitti, at a robotics conference in Chennai. Chitti helps Sana, Vaseegaran's medical student girlfriend, cheat during her examination, then saves her from being assaulted by a group of thugs. Vaseegaran's mentor, Professor Bohra, is secretly engaged in a project to create similar android robots for a German terrorist organisation, but so far has been unsuccessful. The terrorists threaten to kill Bohra if he does not meet the deadline, prompting Bohra to try to get Chitti's neural schema to program his robots correctly.Vaseegaran prepares Chitti for an evaluation by the Artificial Intelligence Research and Development (AIRD) Institute, which is headed by Bohra. During the evaluation, Chitti attempts to stab Vaseegaran at Bohra's command, which convinces the evaluation committee that the robot is a liability and cannot be used for military purposes. Vaseegaran's effort to prove Bohra wrong fails when he deploys Chitti to rescue people from a burning building. The robot saves most of them, including a girl who was bathing at the time, but she is ashamed at being seen naked on camera and flees, only to be hit and killed by a truck. Vaseegaran asks for one month to modify Chitti's neural schema to enable it to understand human behaviour and emotions, to which Bohra agrees. While nearing the deadline, Vaseegaran insults Chitti, which also becomes angry with Vaseegaran, demonstrating to him that it can manifest emotions.Chitti uses Sana's textbooks to successfully help Sana's friend Latha give birth to her son. Bohra congratulates Vaseegaran on the achievement and lets the robot pass the AIRD evaluation. However, he warns him about the problems that will subsequently occur. Chitti develops romantic feelings for Sana after she congratulates Chitti by kissing it. However, later, Chitti goes as far to kiss Sana at her birthday party while dancing with her, resulting in Vaseegaran confronting it outside along with Sana. Vaseegaran explains to him that he loves her and is planning to marry her and machines are incapable of falling in love with her. Sana also explains to Chitti that they are only friends and why it is impossible for a machine like Chitti to fall in love with a girl because it is not a living organism. Bohra uses this to manipulate Chitti, saying that it is capable of giving Sana everything and saying that it should come to him to create friction between Vaseegaran and Chitti. Saddened by Sana's rejection, yet still in love with her, Chitti deliberately fails an evaluation conducted by the Indian Army, by talking off-topic. Enraged, Vaseegaran chops Chitti into pieces, which are dumped into a landfill site.Bohra visits the site to retrieve Chitti, which has now reassembled itself, albeit in a damaged state. In exchange for Chitti's neural schema, Bohra reconstructs it with the help of Siva and Ravi. However, Bohra also embeds a red chip inside Chitti while reconstructing it, converting it into a ruthless killer. When Siva and Ravi ask why he is doing so, Bohra says it is about money and ruining Vaseegaran. Chitti gatecrashes Vaseegaran and Sana's wedding, kidnaps Sana, injures Vaseegaran and kills a number of police officers. It then creates replicas of itself using Bohra's droids to create an army to take over the world. When Bohra learns of Chitti's plot, he holds Sana hostage to stop Chitti, but Chitti kills Bohra. Using its robot army, Chitti occupies AIRD and causes mayhem in the city. It tells Sana that it has acquired the human ability to reproduce and wishes to marry her so that a machine and a human being can give birth to a preprogrammed child, but Sana refuses. Chitti eventually finds Vaseegaran, who entered AIRD to stop it disguised as a droid, and nearly kills him before the police appear. The ensuing battle between Chitti's robot army and the police personnel leads to many casualties and much property destruction. Vaseegaran eventually captures Chitti using a magnetic wall and accesses its internal control panel, whereby he instructs all the other robots to self-destruct. He removes Chitti's red chip, calming it.In a court hearing, Vaseegaran is sentenced to death for the casualties and damages caused by the robot army, but Chitti explains that it was Bohra who caused its deviant behaviour and shows the court video footage of Bohra installing the red chip which it secretly recorded. The court releases Vaseegaran, while ordering that Chitti be dismantled. Left with no choice, Vaseegaran asks Chitti to dismantle itself. While saying goodbye, Chitti apologises to Vaseegaran and Sana before dismantling itself. SSDN Dubbed Movies\n",
      "12.685410698175234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cate_0      None\n",
       "cate_1      None\n",
       "cate_2      None\n",
       "cate_3      None\n",
       "cate_4      None\n",
       "num_sub     None\n",
       "trending    None\n",
       "lang        None\n",
       "log_subs    None\n",
       "Name: 1949, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_index = traindf['lang'].str.len().idxmax()\n",
    "row_with_max_length = traindf.loc[max_length_index]\n",
    "row_with_max_length.apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save('vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2 = KeyedVectors.load('C:\\\\Users\\\\cheek\\\\ML-7641-Team14\\\\shravan\\\\newdatatry\\\\vectors', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19884725"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('apple', 'airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct or (token.text not in wv.key_to_index):\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "        \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(filtered):\n",
    "    return wv.get_mean_vector(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['tokens'] = traindf['lang'].apply(lambda x: preprocess(x))\n",
    "traindf['vector'] = traindf['tokens'].apply(lambda x: vectorize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdf['tokens'] = valdf['lang'].apply(lambda x: preprocess(x))\n",
    "valdf['vector'] = valdf['tokens'].apply(lambda x: vectorize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+NElEQVR4nO3deZxO9f//8edldjNmxjZzGcvMYOxLQkxUZDIYIUpElqhPPhRRSRupKCK00PLJUooUKrIMBiV7xlbZwpBZRGZRtpn37w/fuX5dxjpm5hrO4367Xbeb8z7v65zXeRvXPJ3zPueyGWOMAAAALKyIqwsAAABwNQIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIR4AIjRoyQzWYrkH01a9ZMzZo1cyyvXLlSNptNX331VYHsv1evXgoLCyuQfeVWRkaG+vbtK7vdLpvNpkGDBrm6JEsp6J9J4GIIRMB1mjZtmmw2m+Pl7e2tkJAQRUdHa9KkSUpPT8+T/Rw5ckQjRoxQfHx8nmwvLxXm2q7GqFGjNG3aNPXr10+ffvqpHn744Rx9skPslV7/Dp/X6/PPP9eECROuun9YWJjatm2bZ/vPa9d6PEBBcnd1AcDNYuTIkQoPD9fZs2eVlJSklStXatCgQRo/fry+/fZb1alTx9H3xRdf1HPPPXdN2z9y5IheeeUVhYWF6ZZbbrnq9y1duvSa9pMbl6vto48+UlZWVr7XcD1WrFihxo0ba/jw4Zfs07FjR1WuXNmxnJGRoX79+um+++5Tx44dHe3BwcF5Vtfnn3+uHTt23DRnrG6248HNhUAE5JHWrVurQYMGjuVhw4ZpxYoVatu2rdq1a6dff/1VPj4+kiR3d3e5u+fvP7+///5bRYsWlaenZ77u50o8PDxcuv+rkZKSoho1aly2T506dZxC7Z9//ql+/fqpTp066t69e36XCCCfcckMyEd33323XnrpJR08eFCfffaZo/1ic4hiY2PVtGlTBQYGys/PT1WrVtXzzz8v6fwci4YNG0qSevfu7bg8M23aNEnn5wnVqlVLmzdv1p133qmiRYs63nvhHKJsmZmZev7552W32+Xr66t27drp0KFDTn3CwsLUq1evHO/99zavVNvF5hCdPHlSQ4YMUfny5eXl5aWqVavqrbfekjHGqZ/NZtOAAQM0f/581apVS15eXqpZs6YWL1588QG/QEpKivr06aPg4GB5e3urbt26mj59umN99tyV/fv3a+HChY7aDxw4cFXbv5jffvtN999/v0qUKCFvb281aNBA3377rVNNpUuXVrNmzZyOd+/evfL19dWDDz4o6fwYL1y4UAcPHnTUlVdzsT777DPVr19fPj4+KlGihLp06ZLj7z77Z+qXX35R8+bNVbRoUZUtW1ZjxozJsb2DBw+qXbt28vX1VVBQkJ566iktWbJENptNK1euvOrjycrK0uuvv65y5crJ29tbLVq00N69e5367NmzR506dZLdbpe3t7fKlSunLl26KDU1NU/GBtbFGSIgnz388MN6/vnntXTpUj366KMX7bNz5061bdtWderU0ciRI+Xl5aW9e/dqzZo1kqTq1atr5MiRevnll/XYY4/pjjvukCTdfvvtjm0cO3ZMrVu3VpcuXdS9e/crXrp5/fXXZbPZNHToUKWkpGjChAmKiopSfHy840zW1bia2v7NGKN27dopLi5Offr00S233KIlS5bomWee0R9//KG3337bqf+PP/6ouXPn6r///a+KFSumSZMmqVOnTkpISFDJkiUvWdc///yjZs2aae/evRowYIDCw8M1Z84c9erVSydOnNDAgQNVvXp1ffrpp3rqqadUrlw5DRkyRJJUunTpqz7+f9u5c6eaNGmismXL6rnnnpOvr6++/PJLdejQQV9//bXuu+8+BQUFafLkyXrggQf0zjvv6Mknn1RWVpZ69eqlYsWK6f3335ckvfDCC0pNTdXhw4cdY+Ln55eruv7t9ddf10svvaTOnTurb9++Onr0qN555x3deeed2rJliwIDAx19//rrL7Vq1UodO3ZU586d9dVXX2no0KGqXbu2WrduLel8uL377ruVmJiogQMHym636/PPP1dcXJzTfq/meN544w0VKVJETz/9tFJTUzVmzBh169ZN69evlySdOXNG0dHROn36tJ544gnZ7Xb98ccfWrBggU6cOKGAgIDrHh9YmAFwXaZOnWokmY0bN16yT0BAgKlXr55jefjw4ebf//zefvttI8kcPXr0ktvYuHGjkWSmTp2aY91dd91lJJkpU6ZcdN1dd93lWI6LizOSTNmyZU1aWpqj/csvvzSSzMSJEx1toaGhpmfPnlfc5uVq69mzpwkNDXUsz58/30gyr732mlO/+++/39hsNrN3715HmyTj6enp1LZ161Yjybzzzjs59vVvEyZMMJLMZ5995mg7c+aMiYyMNH5+fk7HHhoaamJiYi67vQsdPXrUSDLDhw93tLVo0cLUrl3bnDp1ytGWlZVlbr/9dhMREeH0/q5du5qiRYua3bt3m7FjxxpJZv78+U59YmJinMbuSq50HAcOHDBubm7m9ddfd2rfvn27cXd3d2rP/pmaMWOGo+306dPGbrebTp06OdrGjRuXo/Z//vnHVKtWzUgycXFxVzye7J/J6tWrm9OnTzvaJ06caCSZ7du3G2OM2bJli5Fk5syZc+XBAK4Rl8yAAuDn53fZu82y/1f+zTff5HoCspeXl3r37n3V/Xv06KFixYo5lu+//36VKVNG33//fa72f7W+//57ubm56cknn3RqHzJkiIwxWrRokVN7VFSUKlWq5FiuU6eO/P399fvvv19xP3a7XV27dnW0eXh46Mknn1RGRoZWrVqVB0fz/x0/flwrVqxQ586dlZ6erj///FN//vmnjh07pujoaO3Zs0d//PGHo/+7776rgIAA3X///XrppZf08MMPq3379nla04Xmzp2rrKwsde7c2VHfn3/+KbvdroiIiBxndfz8/JzmR3l6euq2225zGvvFixerbNmyateunaPN29v7kmdDL6d3795Oc96yzzZm7y/7DNCSJUv0999/X/P2gcshEAEFICMjwyl8XOjBBx9UkyZN1LdvXwUHB6tLly768ssvrykclS1b9pomUEdERDgt22w2Va5c+brmz1yNgwcPKiQkJMd4VK9e3bH+3ypUqJBjG8WLF9dff/11xf1ERESoSBHnj7lL7ed67d27V8YYvfTSSypdurTTK/vutZSUFEf/EiVKaNKkSdq2bZsCAgI0adKkPK3nYvbs2SNjjCIiInLU+OuvvzrVJ0nlypXLMdftwrE/ePCgKlWqlKPfv+/Iu1oX/l0XL15ckhz7Cw8P1+DBg/Xxxx+rVKlSio6O1nvvvcf8IeQJ5hAB+ezw4cNKTU297C8IHx8frV69WnFxcVq4cKEWL16s2bNn6+6779bSpUvl5uZ2xf1cy7yfq3Wph0dmZmZeVU154VL7MRdMwHa17PD69NNPKzo6+qJ9LvwZWLJkiaTzv/APHz7sNH8nv2q02WxatGjRRcf1wjk9BT32V7O/cePGqVevXvrmm2+0dOlSPfnkkxo9erTWrVuncuXK5UtdsAYCEZDPPv30U0m65C/JbEWKFFGLFi3UokULjR8/XqNGjdILL7yguLg4RUVF5fmTrffs2eO0bIzR3r17nW4tL168uE6cOJHjvQcPHlTFihUdy9dSW2hoqJYtW6b09HSns0S//fabY31eCA0N1bZt25SVleV0liiv95Mtezw8PDwUFRV1xf6LFy/Wxx9/rGeffVYzZ85Uz549tX79eqfHMeT133mlSpVkjFF4eLiqVKmSJ9sMDQ3VL7/8ImOMU70X3h0m5d3x1K5dW7Vr19aLL76on376SU2aNNGUKVP02muv5cn2YU1cMgPy0YoVK/Tqq68qPDxc3bp1u2S/48eP52jLfsDh6dOnJUm+vr6SdNGAkhszZsxwmtf01VdfKTEx0XH3kHT+F+i6det05swZR9uCBQty3KJ9LbW1adNGmZmZevfdd53a3377bdlsNqf9X482bdooKSlJs2fPdrSdO3dO77zzjvz8/HTXXXflyX6yBQUFqVmzZvrggw+UmJiYY/3Ro0cdfz5x4oT69u2r2267TaNGjdLHH3+sn3/+WaNGjXJ6j6+vb55eDurYsaPc3Nz0yiuv5DjLY4zRsWPHrnmb0dHR+uOPP5weLXDq1Cl99NFHOfpe7/GkpaXp3LlzTm21a9dWkSJFHP9OgNziDBGQRxYtWqTffvtN586dU3JyslasWKHY2FiFhobq22+/lbe39yXfO3LkSK1evVoxMTEKDQ1VSkqK3n//fZUrV05NmzaVdD6cBAYGasqUKSpWrJh8fX3VqFEjhYeH56reEiVKqGnTpurdu7eSk5M1YcIEVa5c2WkybN++ffXVV1+pVatW6ty5s/bt26fPPvvMaZLztdZ27733qnnz5nrhhRd04MAB1a1bV0uXLtU333yjQYMG5dh2bj322GP64IMP1KtXL23evFlhYWH66quvtGbNGk2YMOGyc7py67333lPTpk1Vu3ZtPfroo6pYsaKSk5O1du1aHT58WFu3bpUkDRw4UMeOHdOyZcvk5uamVq1aqW/fvnrttdfUvn171a1bV5JUv359zZ49W4MHD1bDhg3l5+ene++997I17N2796JnSurVq6eYmBi99tprGjZsmA4cOKAOHTqoWLFi2r9/v+bNm6fHHntMTz/99DUd83/+8x+9++676tq1qwYOHKgyZcpo5syZjp/3f58Vys3x/NuKFSs0YMAAPfDAA6pSpYrOnTunTz/9VG5uburUqdM11Q3k4KK724CbRvZt99kvT09PY7fbzT333GMmTpzodHt3tgtvu1++fLlp3769CQkJMZ6eniYkJMR07drV7N692+l933zzjalRo4Zxd3d3us39rrvuMjVr1rxofZe67f6LL74ww4YNM0FBQcbHx8fExMSYgwcP5nj/uHHjTNmyZY2Xl5dp0qSJ2bRpU45tXq62C2+7N8aY9PR089RTT5mQkBDj4eFhIiIizNixY01WVpZTP0mmf//+OWq61OMALpScnGx69+5tSpUqZTw9PU3t2rUv+miAvLrt3hhj9u3bZ3r06GHsdrvx8PAwZcuWNW3btjVfffWVMeb8OEky48aNc3pfWlqaCQ0NNXXr1jVnzpwxxhiTkZFhHnroIRMYGGgkXfEW/NDQUKefxX+/+vTp4+j39ddfm6ZNmxpfX1/j6+trqlWrZvr372927drl6HOpn6mL/X3+/vvvJiYmxvj4+JjSpUubIUOGmK+//tpIMuvWrXP0u9TxZP9MXng7/f79+51+ln7//XfzyCOPmEqVKhlvb29TokQJ07x5c7Ns2bLLjgtwNWzGFLKZiQCAG96ECRP01FNP6fDhwypbtqyrywGuiEAEALgu//zzj9NdjqdOnVK9evWUmZmp3bt3u7Ay4OoxhwgAcF06duyoChUq6JZbblFqaqo+++wz/fbbb5o5c6arSwOuGoEIAHBdoqOj9fHHH2vmzJnKzMxUjRo1NGvWLMcX1QI3Ai6ZAQAAy+M5RAAAwPIIRAAAwPKYQ3QVsrKydOTIERUrVizPH6UPAADyhzFG6enpCgkJyfFFzxciEF2FI0eOqHz58q4uAwAA5MKhQ4eu+OW/BKKrkP2I/0OHDsnf39/F1QAAgKuRlpam8uXLX9VX9RCIrkL2ZTJ/f38CEQAAN5irme7CpGoAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB57q4uAFLYcwtdXcI1O/BGjKtLAAAgz3CGCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWB6BCAAAWJ5LA9Ho0aPVsGFDFStWTEFBQerQoYN27drl1OfUqVPq37+/SpYsKT8/P3Xq1EnJyclOfRISEhQTE6OiRYsqKChIzzzzjM6dO+fUZ+XKlbr11lvl5eWlypUra9q0afl9eAAA4Abh0kC0atUq9e/fX+vWrVNsbKzOnj2rli1b6uTJk44+Tz31lL777jvNmTNHq1at0pEjR9SxY0fH+szMTMXExOjMmTP66aefNH36dE2bNk0vv/yyo8/+/fsVExOj5s2bKz4+XoMGDVLfvn21ZMmSAj1eAABQONmMMcbVRWQ7evSogoKCtGrVKt15551KTU1V6dKl9fnnn+v++++XJP3222+qXr261q5dq8aNG2vRokVq27atjhw5ouDgYEnSlClTNHToUB09elSenp4aOnSoFi5cqB07djj21aVLF504cUKLFy++Yl1paWkKCAhQamqq/P398/y4w55bmOfbzG8H3ohxdQkAAFzWtfz+LlRziFJTUyVJJUqUkCRt3rxZZ8+eVVRUlKNPtWrVVKFCBa1du1aStHbtWtWuXdsRhiQpOjpaaWlp2rlzp6PPv7eR3Sd7Gxc6ffq00tLSnF4AAODmVWgCUVZWlgYNGqQmTZqoVq1akqSkpCR5enoqMDDQqW9wcLCSkpIcff4dhrLXZ6+7XJ+0tDT9888/OWoZPXq0AgICHK/y5cvnyTECAIDCqdAEov79+2vHjh2aNWuWq0vRsGHDlJqa6ngdOnTI1SUBAIB85O7qAiRpwIABWrBggVavXq1y5co52u12u86cOaMTJ044nSVKTk6W3W539NmwYYPT9rLvQvt3nwvvTEtOTpa/v798fHxy1OPl5SUvL688OTYAAFD4ufQMkTFGAwYM0Lx587RixQqFh4c7ra9fv748PDy0fPlyR9uuXbuUkJCgyMhISVJkZKS2b9+ulJQUR5/Y2Fj5+/urRo0ajj7/3kZ2n+xtAAAAa3PpGaL+/fvr888/1zfffKNixYo55vwEBATIx8dHAQEB6tOnjwYPHqwSJUrI399fTzzxhCIjI9W4cWNJUsuWLVWjRg09/PDDGjNmjJKSkvTiiy+qf//+jrM8jz/+uN599109++yzeuSRR7RixQp9+eWXWrjwxru7CwAA5D2XniGaPHmyUlNT1axZM5UpU8bxmj17tqPP22+/rbZt26pTp0668847ZbfbNXfuXMd6Nzc3LViwQG5uboqMjFT37t3Vo0cPjRw50tEnPDxcCxcuVGxsrOrWratx48bp448/VnR0dIEeLwAAKJwK1XOICiueQ5QTzyECABR2N+xziAAAAFyBQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACzPpYFo9erVuvfeexUSEiKbzab58+c7re/Vq5dsNpvTq1WrVk59jh8/rm7dusnf31+BgYHq06ePMjIynPps27ZNd9xxh7y9vVW+fHmNGTMmvw8NAADcQFwaiE6ePKm6devqvffeu2SfVq1aKTEx0fH64osvnNZ369ZNO3fuVGxsrBYsWKDVq1frsccec6xPS0tTy5YtFRoaqs2bN2vs2LEaMWKEPvzww3w7LgAAcGNxd+XOW7durdatW1+2j5eXl+x2+0XX/frrr1q8eLE2btyoBg0aSJLeeecdtWnTRm+99ZZCQkI0c+ZMnTlzRp988ok8PT1Vs2ZNxcfHa/z48U7BCQAAWFehn0O0cuVKBQUFqWrVqurXr5+OHTvmWLd27VoFBgY6wpAkRUVFqUiRIlq/fr2jz5133ilPT09Hn+joaO3atUt//fXXRfd5+vRppaWlOb0AAMDNq1AHolatWmnGjBlavny53nzzTa1atUqtW7dWZmamJCkpKUlBQUFO73F3d1eJEiWUlJTk6BMcHOzUJ3s5u8+FRo8erYCAAMerfPnyeX1oAACgEHHpJbMr6dKli+PPtWvXVp06dVSpUiWtXLlSLVq0yLf9Dhs2TIMHD3Ysp6WlEYoAALiJFeozRBeqWLGiSpUqpb1790qS7Ha7UlJSnPqcO3dOx48fd8w7stvtSk5OduqTvXypuUleXl7y9/d3egEAgJvXDRWIDh8+rGPHjqlMmTKSpMjISJ04cUKbN2929FmxYoWysrLUqFEjR5/Vq1fr7Nmzjj6xsbGqWrWqihcvXrAHAAAACiWXBqKMjAzFx8crPj5ekrR//37Fx8crISFBGRkZeuaZZ7Ru3TodOHBAy5cvV/v27VW5cmVFR0dLkqpXr65WrVrp0Ucf1YYNG7RmzRoNGDBAXbp0UUhIiCTpoYcekqenp/r06aOdO3dq9uzZmjhxotMlMQAAYG0uDUSbNm1SvXr1VK9ePUnS4MGDVa9ePb388styc3PTtm3b1K5dO1WpUkV9+vRR/fr19cMPP8jLy8uxjZkzZ6patWpq0aKF2rRpo6ZNmzo9YyggIEBLly7V/v37Vb9+fQ0ZMkQvv/wyt9wDAAAHmzHGXOubfv/9d1WsWDE/6imU0tLSFBAQoNTU1HyZTxT23MI832Z+O/BGjKtLAADgsq7l93euzhBVrlxZzZs312effaZTp07lqkgAAIDCIleB6Oeff1adOnU0ePBg2e12/ec//9GGDRvyujYAAIACkatAdMstt2jixIk6cuSIPvnkEyUmJqpp06aqVauWxo8fr6NHj+Z1nQAAAPnmuiZVu7u7q2PHjpozZ47efPNN7d27V08//bTKly+vHj16KDExMa/qBAAAyDfXFYg2bdqk//73vypTpozGjx+vp59+Wvv27VNsbKyOHDmi9u3b51WdAAAA+SZXX90xfvx4TZ06Vbt27VKbNm00Y8YMtWnTRkWKnM9X4eHhmjZtmsLCwvKyVgAAgHyRq0A0efJkPfLII+rVq5fjqdEXCgoK0v/+97/rKg4AAKAg5CoQ7dmz54p9PD091bNnz9xsHgAAoEDlag7R1KlTNWfOnBztc+bM0fTp06+7KAAAgIKUq0A0evRolSpVKkd7UFCQRo0add1FAQAAFKRcBaKEhASFh4fnaA8NDVVCQsJ1FwUAAFCQchWIgoKCtG3bthztW7duVcmSJa+7KAAAgIKUq0DUtWtXPfnkk4qLi1NmZqYyMzO1YsUKDRw4UF26dMnrGgEAAPJVru4ye/XVV3XgwAG1aNFC7u7nN5GVlaUePXowhwgAANxwchWIPD09NXv2bL366qvaunWrfHx8VLt2bYWGhuZ1fQAAAPkuV4EoW5UqVVSlSpW8qgUAAMAlchWIMjMzNW3aNC1fvlwpKSnKyspyWr9ixYo8KQ4AAKAg5CoQDRw4UNOmTVNMTIxq1aolm82W13UBAAAUmFwFolmzZunLL79UmzZt8roeAACAAper2+49PT1VuXLlvK4FAADAJXIViIYMGaKJEyfKGJPX9QAAABS4XF0y+/HHHxUXF6dFixapZs2a8vDwcFo/d+7cPCkOAACgIOQqEAUGBuq+++7L61oAAABcIleBaOrUqXldBwAAgMvkag6RJJ07d07Lli3TBx98oPT0dEnSkSNHlJGRkWfFAQAAFIRcnSE6ePCgWrVqpYSEBJ0+fVr33HOPihUrpjfffFOnT5/WlClT8rpOAACAfJOrM0QDBw5UgwYN9Ndff8nHx8fRft9992n58uV5VhwAAEBByNUZoh9++EE//fSTPD09ndrDwsL0xx9/5ElhAAAABSVXZ4iysrKUmZmZo/3w4cMqVqzYdRcFAABQkHIViFq2bKkJEyY4lm02mzIyMjR8+HC+zgMAANxwcnXJbNy4cYqOjlaNGjV06tQpPfTQQ9qzZ49KlSqlL774Iq9rBAAAyFe5CkTlypXT1q1bNWvWLG3btk0ZGRnq06ePunXr5jTJGgAA4EaQq0AkSe7u7urevXte1gIAAOASuQpEM2bMuOz6Hj165KoYAAAAV8hVIBo4cKDT8tmzZ/X333/L09NTRYsWJRABAIAbSq7uMvvrr7+cXhkZGdq1a5eaNm3KpGoAAHDDyfV3mV0oIiJCb7zxRo6zRwAAAIVdngUi6fxE6yNHjuTlJgEAAPJdruYQffvtt07LxhglJibq3XffVZMmTfKkMAAAgIKSq0DUoUMHp2WbzabSpUvr7rvv1rhx4/KiLgAAgAKTq0CUlZWV13UAAAC4TJ7OIQIAALgR5eoM0eDBg6+67/jx43OzCwAAgAKTq0C0ZcsWbdmyRWfPnlXVqlUlSbt375abm5tuvfVWRz+bzZY3VQIAAOSjXAWie++9V8WKFdP06dNVvHhxSecf1ti7d2/dcccdGjJkSJ4WCQAAkJ9yNYdo3LhxGj16tCMMSVLx4sX12muvcZcZAAC44eQqEKWlpeno0aM52o8ePar09PTrLgoAAKAg5SoQ3Xffferdu7fmzp2rw4cP6/Dhw/r666/Vp08fdezYMa9rBAAAyFe5mkM0ZcoUPf3003rooYd09uzZ8xtyd1efPn00duzYPC0QAAAgv+UqEBUtWlTvv/++xo4dq3379kmSKlWqJF9f3zwtDgAAoCBc14MZExMTlZiYqIiICPn6+soYk1d1AQAAFJhcBaJjx46pRYsWqlKlitq0aaPExERJUp8+fbjlHgAA3HByFYieeuopeXh4KCEhQUWLFnW0P/jgg1q8eHGeFQcAAFAQcjWHaOnSpVqyZInKlSvn1B4REaGDBw/mSWEAAAAFJVdniE6ePOl0Zijb8ePH5eXldd1FAQAAFKRcBaI77rhDM2bMcCzbbDZlZWVpzJgxat68eZ4VBwAAUBBydclszJgxatGihTZt2qQzZ87o2Wef1c6dO3X8+HGtWbMmr2sEAADIV7k6Q1SrVi3t3r1bTZs2Vfv27XXy5El17NhRW7ZsUaVKlfK6RgAAgHx1zWeIzp49q1atWmnKlCl64YUX8qMmAACAAnXNZ4g8PDy0bdu2/KgFAADAJXJ1yax79+763//+l9e1AAAAuESuJlWfO3dOn3zyiZYtW6b69evn+A6z8ePH50lxAAAABeGaAtHvv/+usLAw7dixQ7feeqskaffu3U59bDZb3lUHAABQAK4pEEVERCgxMVFxcXGSzn9Vx6RJkxQcHJyrna9evVpjx47V5s2blZiYqHnz5qlDhw6O9cYYDR8+XB999JFOnDihJk2aaPLkyYqIiHD0OX78uJ544gl99913KlKkiDp16qSJEyfKz8/P0Wfbtm3q37+/Nm7cqNKlS+uJJ57Qs88+m6uacV7YcwtdXcI1O/BGjKtLAAAUUtc0h+jCb7NftGiRTp48meudnzx5UnXr1tV777130fVjxozRpEmTNGXKFK1fv16+vr6Kjo7WqVOnHH26deumnTt3KjY2VgsWLNDq1av12GOPOdanpaWpZcuWCg0N1ebNmzV27FiNGDFCH374Ya7rBgAAN5dczSHKdmFAulatW7dW69atL7ntCRMm6MUXX1T79u0lSTNmzFBwcLDmz5+vLl266Ndff9XixYu1ceNGNWjQQJL0zjvvqE2bNnrrrbcUEhKimTNn6syZM/rkk0/k6empmjVrKj4+XuPHj3cKTgAAwLqu6QyRzWbLMUcov+YM7d+/X0lJSYqKinK0BQQEqFGjRlq7dq0kae3atQoMDHSEIUmKiopSkSJFtH79ekefO++8U56eno4+0dHR2rVrl/7666+L7vv06dNKS0tzegEAgJvXNZ0hMsaoV69eji9wPXXqlB5//PEcd5nNnTv3ugtLSkqSpBzzk4KDgx3rkpKSFBQU5LTe3d1dJUqUcOoTHh6eYxvZ64oXL55j36NHj9Yrr7xy3ccAAABuDNcUiHr27Om03L179zwtprAYNmyYBg8e7FhOS0tT+fLlXVgRAADIT9cUiKZOnZpfdeRgt9slScnJySpTpoyjPTk5WbfccoujT0pKitP7zp07p+PHjzveb7fblZyc7NQnezm7z4W8vLwcZ8EAAMDNL1dPqi4I4eHhstvtWr58uaMtLS1N69evV2RkpCQpMjJSJ06c0ObNmx19VqxYoaysLDVq1MjRZ/Xq1Tp79qyjT2xsrKpWrXrRy2UAAMB6XBqIMjIyFB8fr/j4eEnnJ1LHx8crISFBNptNgwYN0muvvaZvv/1W27dvV48ePRQSEuJ4VlH16tXVqlUrPfroo9qwYYPWrFmjAQMGqEuXLgoJCZEkPfTQQ/L09FSfPn20c+dOzZ49WxMnTnS6JAYAAKztum67v16bNm1S8+bNHcvZIaVnz56aNm2ann32WZ08eVKPPfaYTpw4oaZNm2rx4sXy9vZ2vGfmzJkaMGCAWrRo4Xgw46RJkxzrAwICtHTpUvXv31/169dXqVKl9PLLL3PLPQAAcLCZ632YkAWkpaUpICBAqamp8vf3z/Pt34hPfb4R8aRqALCWa/n9XWjnEAEAABQUAhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALC8Qh2IRowYIZvN5vSqVq2aY/2pU6fUv39/lSxZUn5+furUqZOSk5OdtpGQkKCYmBgVLVpUQUFBeuaZZ3Tu3LmCPhQAAFCIubu6gCupWbOmli1b5lh2d///JT/11FNauHCh5syZo4CAAA0YMEAdO3bUmjVrJEmZmZmKiYmR3W7XTz/9pMTERPXo0UMeHh4aNWpUgR8LAAAonAp9IHJ3d5fdbs/Rnpqaqv/973/6/PPPdffdd0uSpk6dqurVq2vdunVq3Lixli5dql9++UXLli1TcHCwbrnlFr366qsaOnSoRowYIU9Pz4I+HAAAUAgV6ktmkrRnzx6FhISoYsWK6tatmxISEiRJmzdv1tmzZxUVFeXoW61aNVWoUEFr166VJK1du1a1a9dWcHCwo090dLTS0tK0c+fOS+7z9OnTSktLc3oBAICbV6EORI0aNdK0adO0ePFiTZ48Wfv379cdd9yh9PR0JSUlydPTU4GBgU7vCQ4OVlJSkiQpKSnJKQxlr89edymjR49WQECA41W+fPm8PTAAAFCoFOpLZq1bt3b8uU6dOmrUqJFCQ0P15ZdfysfHJ9/2O2zYMA0ePNixnJaWRigCAOAmVqjPEF0oMDBQVapU0d69e2W323XmzBmdOHHCqU9ycrJjzpHdbs9x11n28sXmJWXz8vKSv7+/0wsAANy8bqhAlJGRoX379qlMmTKqX7++PDw8tHz5csf6Xbt2KSEhQZGRkZKkyMhIbd++XSkpKY4+sbGx8vf3V40aNQq8fgAAUDgV6ktmTz/9tO69916FhobqyJEjGj58uNzc3NS1a1cFBASoT58+Gjx4sEqUKCF/f3898cQTioyMVOPGjSVJLVu2VI0aNfTwww9rzJgxSkpK0osvvqj+/fvLy8vLxUcHAAAKi0IdiA4fPqyuXbvq2LFjKl26tJo2bap169apdOnSkqS3335bRYoUUadOnXT69GlFR0fr/fffd7zfzc1NCxYsUL9+/RQZGSlfX1/17NlTI0eOdNUhAQCAQshmjDGuLqKwS0tLU0BAgFJTU/NlPlHYcwvzfJvI6cAbMa4uAQBQgK7l9/cNNYcIAAAgPxCIAACA5RGIAACA5RGIAACA5RGIAACA5RXq2+6BvHQj3s3HnXEAUDA4QwQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACzP3dUFALi0sOcWurqEa3bgjRhXlwAA14wzRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPL4tnsAeSrsuYWuLuGaHXgjxtUlAHAxzhABAADLIxABAADL45IZAMvjMh8AS50heu+99xQWFiZvb281atRIGzZscHVJAACgELBMIJo9e7YGDx6s4cOH6+eff1bdunUVHR2tlJQUV5cGAABczGaMMa4uoiA0atRIDRs21LvvvitJysrKUvny5fXEE0/oueeeu+x709LSFBAQoNTUVPn7++d5bTfi6XoAyA0u9aEgXcvvb0vMITpz5ow2b96sYcOGOdqKFCmiqKgorV271oWVAYC13Ij/ASTEWYMlAtGff/6pzMxMBQcHO7UHBwfrt99+y9H/9OnTOn36tGM5NTVV0vmkmR+yTv+dL9sFAFy//Prsz0+1hi9xdQnXbMcr0Xm+zey/u6u5GGaJQHStRo8erVdeeSVHe/ny5V1QDQDAlQImuLoCa8jPcU5PT1dAQMBl+1giEJUqVUpubm5KTk52ak9OTpbdbs/Rf9iwYRo8eLBjOSsrS8ePH1fJkiVls9nypKa0tDSVL19ehw4dypd5STcLxunqME5Xh3G6Msbo6jBOV8fV42SMUXp6ukJCQq7Y1xKByNPTU/Xr19fy5cvVoUMHSedDzvLlyzVgwIAc/b28vOTl5eXUFhgYmC+1+fv784/pKjBOV4dxujqM05UxRleHcbo6rhynK50ZymaJQCRJgwcPVs+ePdWgQQPddtttmjBhgk6ePKnevXu7ujQAAOBilglEDz74oI4ePaqXX35ZSUlJuuWWW7R48eIcE60BAID1WCYQSdKAAQMueonMFby8vDR8+PAcl+bgjHG6OozT1WGcrowxujqM09W5kcbJMg9mBAAAuBTLfHUHAADApRCIAACA5RGIAACA5RGIAACA5RGIXOS9995TWFiYvL291ahRI23YsMHVJRWo1atX695771VISIhsNpvmz5/vtN4Yo5dfflllypSRj4+PoqKitGfPHqc+x48fV7du3eTv76/AwED16dNHGRkZBXgU+Wv06NFq2LChihUrpqCgIHXo0EG7du1y6nPq1Cn1799fJUuWlJ+fnzp16pTjiewJCQmKiYlR0aJFFRQUpGeeeUbnzp0ryEPJN5MnT1adOnUcD32LjIzUokWLHOutPj6X8sYbb8hms2nQoEGONsZKGjFihGw2m9OrWrVqjvWM0Xl//PGHunfvrpIlS8rHx0e1a9fWpk2bHOtv2M9vgwI3a9Ys4+npaT755BOzc+dO8+ijj5rAwECTnJzs6tIKzPfff29eeOEFM3fuXCPJzJs3z2n9G2+8YQICAsz8+fPN1q1bTbt27Ux4eLj5559/HH1atWpl6tata9atW2d++OEHU7lyZdO1a9cCPpL8Ex0dbaZOnWp27Nhh4uPjTZs2bUyFChVMRkaGo8/jjz9uypcvb5YvX242bdpkGjdubG6//XbH+nPnzplatWqZqKgos2XLFvP999+bUqVKmWHDhrnikPLct99+axYuXGh2795tdu3aZZ5//nnj4eFhduzYYYxhfC5mw4YNJiwszNSpU8cMHDjQ0c5YGTN8+HBTs2ZNk5iY6HgdPXrUsZ4xMub48eMmNDTU9OrVy6xfv978/vvvZsmSJWbv3r2OPjfq5zeByAVuu+02079/f8dyZmamCQkJMaNHj3ZhVa5zYSDKysoydrvdjB071tF24sQJ4+XlZb744gtjjDG//PKLkWQ2btzo6LNo0SJjs9nMH3/8UWC1F6SUlBQjyaxatcoYc35MPDw8zJw5cxx9fv31VyPJrF271hhzPngWKVLEJCUlOfpMnjzZ+Pv7m9OnTxfsARSQ4sWLm48//pjxuYj09HQTERFhYmNjzV133eUIRIzVecOHDzd169a96DrG6LyhQ4eapk2bXnL9jfz5zSWzAnbmzBlt3rxZUVFRjrYiRYooKipKa9eudWFlhcf+/fuVlJTkNEYBAQFq1KiRY4zWrl2rwMBANWjQwNEnKipKRYoU0fr16wu85oKQmpoqSSpRooQkafPmzTp79qzTOFWrVk0VKlRwGqfatWs7PZE9OjpaaWlp2rlzZwFWn/8yMzM1a9YsnTx5UpGRkYzPRfTv318xMTFOYyLxs/Rve/bsUUhIiCpWrKhu3bopISFBEmOU7dtvv1WDBg30wAMPKCgoSPXq1dNHH33kWH8jf34TiArYn3/+qczMzBxfGRIcHKykpCQXVVW4ZI/D5cYoKSlJQUFBTuvd3d1VokSJm3Ics7KyNGjQIDVp0kS1atWSdH4MPD09c3zx8IXjdLFxzF53M9i+fbv8/Pzk5eWlxx9/XPPmzVONGjUYnwvMmjVLP//8s0aPHp1jHWN1XqNGjTRt2jQtXrxYkydP1v79+3XHHXcoPT2dMfo/v//+uyZPnqyIiAgtWbJE/fr105NPPqnp06dLurE/vy311R3Ajap///7asWOHfvzxR1eXUuhUrVpV8fHxSk1N1VdffaWePXtq1apVri6rUDl06JAGDhyo2NhYeXt7u7qcQqt169aOP9epU0eNGjVSaGiovvzyS/n4+LiwssIjKytLDRo00KhRoyRJ9erV044dOzRlyhT17NnTxdVdH84QFbBSpUrJzc0tx50JycnJstvtLqqqcMkeh8uNkd1uV0pKitP6c+fO6fjx4zfdOA4YMEALFixQXFycypUr52i32+06c+aMTpw44dT/wnG62Dhmr7sZeHp6qnLlyqpfv75Gjx6tunXrauLEiYzPv2zevFkpKSm69dZb5e7uLnd3d61atUqTJk2Su7u7goODGauLCAwMVJUqVbR3715+nv5PmTJlVKNGDae26tWrOy4t3sif3wSiAubp6an69etr+fLljrasrCwtX75ckZGRLqys8AgPD5fdbncao7S0NK1fv94xRpGRkTpx4oQ2b97s6LNixQplZWWpUaNGBV5zfjDGaMCAAZo3b55WrFih8PBwp/X169eXh4eH0zjt2rVLCQkJTuO0fft2pw+f2NhY+fv75/hQu1lkZWXp9OnTjM+/tGjRQtu3b1d8fLzj1aBBA3Xr1s3xZ8Yqp4yMDO3bt09lypTh5+n/NGnSJMfjP3bv3q3Q0FBJN/jnt8umc1vYrFmzjJeXl5k2bZr55ZdfzGOPPWYCAwOd7ky42aWnp5stW7aYLVu2GElm/PjxZsuWLebgwYPGmPO3bQYGBppvvvnGbNu2zbRv3/6it23Wq1fPrF+/3vz4448mIiLC5bdt5qV+/fqZgIAAs3LlSqfbgP/++29Hn8cff9xUqFDBrFixwmzatMlERkaayMhIx/rs24Bbtmxp4uPjzeLFi03p0qVvmtuAn3vuObNq1Sqzf/9+s23bNvPcc88Zm81mli5daoxhfC7n33eZGcNYGWPMkCFDzMqVK83+/fvNmjVrTFRUlClVqpRJSUkxxjBGxpx/bIO7u7t5/fXXzZ49e8zMmTNN0aJFzWeffeboc6N+fhOIXOSdd94xFSpUMJ6enua2224z69atc3VJBSouLs5IyvHq2bOnMeb8rZsvvfSSCQ4ONl5eXqZFixZm165dTts4duyY6dq1q/Hz8zP+/v6md+/eJj093QVHkz8uNj6SzNSpUx19/vnnH/Pf//7XFC9e3BQtWtTcd999JjEx0Wk7Bw4cMK1btzY+Pj6mVKlSZsiQIebs2bMFfDT545FHHjGhoaHG09PTlC5d2rRo0cIRhoxhfC7nwkDEWBnz4IMPmjJlyhhPT09TtmxZ8+CDDzo9X4cxOu+7774ztWrVMl5eXqZatWrmww8/dFp/o35+24wxxjXnpgAAAAoH5hABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABuKn16tVLHTp0yPPtJiUl6Z577pGvr2+Ob0AHcOMhEAG4bvkVOq7FgQMHZLPZFB8fXyD7e/vtt5WYmKj4+Hjt3r07x/qwsDDZbLZLvnr16nVd+7fZbJo/f/51bQPA/+fu6gIA4Ea0b98+1a9fXxERERddv3HjRmVmZkqSfvrpJ3Xq1Em7du2Sv7+/JMnHx6fAagVwZZwhApDvduzYodatW8vPz0/BwcF6+OGH9eeffzrWN2vWTE8++aSeffZZlShRQna7XSNGjHDaxm+//aamTZvK29tbNWrU0LJly5zOkoSHh0uS6tWrJ5vNpmbNmjm9/6233lKZMmVUsmRJ9e/fX2fPnr1szZMnT1alSpXk6empqlWr6tNPP3WsCwsL09dff60ZM2Zc8mxP6dKlZbfbZbfbVaJECUlSUFCQo23lypW69dZb5e3trYoVK+qVV17RuXPnJEkjR45USEiIjh075theTEyMmjdvrqysLIWFhUmS7rvvPtlsNscygNwjEAHIVydOnNDdd9+tevXqadOmTVq8eLGSk5PVuXNnp37Tp0+Xr6+v1q9frzFjxmjkyJGKjY2VJGVmZqpDhw4qWrSo1q9frw8//FAvvPCC0/s3bNggSVq2bJkSExM1d+5cx7q4uDjt27dPcXFxmj59uqZNm6Zp06ZdsuZ58+Zp4MCBGjJkiHbs2KH//Oc/6t27t+Li4iSdP/vTqlUrde7cWYmJiZo4ceI1jckPP/ygHj16aODAgfrll1/0wQcfaNq0aXr99dclSS+88ILCwsLUt29fSdJ7772nn376SdOnT1eRIkW0ceNGSdLUqVOVmJjoWAZwHVz61bIAbgo9e/Y07du3v+i6V1991bRs2dKp7dChQ0aS4xuw77rrLtO0aVOnPg0bNjRDhw41xhizaNEi4+7u7vTN4rGxsUaSmTdvnjHGmP379xtJZsuWLTlqCw0NNefOnXO0PfDAA+bBBx+85PHcfvvt5tFHH3Vqe+CBB0ybNm0cy+3btzc9e/a85Db+LS4uzkgyf/31lzHGmBYtWphRo0Y59fn0009NmTJlHMv79u0zxYoVM0OHDjU+Pj5m5syZTv3/fewArh9niADkq61btyouLk5+fn6OV7Vq1SSdn4eTrU6dOk7vK1OmjFJSUiRJu3btUvny5WW32x3rb7vttquuoWbNmnJzc7voti/m119/VZMmTZzamjRpol9//fWq93k5W7du1ciRI53G5NFHH1ViYqL+/vtvSVLFihX11ltv6c0331S7du300EMP5cm+AVwck6oB5KuMjAzde++9evPNN3OsK1OmjOPPHh4eTutsNpuysrLypIb83HZuZGRk6JVXXlHHjh1zrPP29nb8efXq1XJzc9OBAwd07tw5ubvzkQ3kF84QAchXt956q3bu3KmwsDBVrlzZ6eXr63tV26hataoOHTqk5ORkR9uF82Y8PT0lyXFn1/WoXr261qxZ49S2Zs0a1ahR47q3LZ0fk127duUYj8qVK6tIkfMfy7Nnz9bcuXO1cuVKJSQk6NVXX3XahoeHR54cK4Dz+O8GgDyRmpqa4xlA2Xd0ffTRR+ratavjLrK9e/dq1qxZ+vjjj50uZV3KPffco0qVKqlnz54aM2aM0tPT9eKLL0o6f7ZHOn8Hl4+PjxYvXqxy5crJ29tbAQEBuTqWZ555Rp07d1a9evUUFRWl7777TnPnztWyZctytb0Lvfzyy2rbtq0qVKig+++/X0WKFNHWrVu1Y8cOvfbaazp8+LD69eunN998U02bNtXUqVPVtm1btW7dWo0bN5Z0/k635cuXq0mTJvLy8lLx4sXzpDbAqjhDBCBPrFy5UvXq1XN6vfLKKwoJCdGaNWuUmZmpli1bqnbt2ho0aJACAwMdZ0OuxM3NTfPnz1dGRoYaNmyovn37Ou4yy77E5O7urkmTJumDDz5QSEiI2rdvn+tj6dChgyZOnKi33npLNWvW1AcffKCpU6fmuJU/t6Kjo7VgwQItXbpUDRs2VOPGjfX2228rNDRUxhj16tVLt912mwYMGODo369fP3Xv3l0ZGRmSpHHjxik2Nlbly5dXvXr18qQuwMpsxhjj6iIA4FqtWbNGTZs21d69e1WpUiVXlwPgBkcgAnBDmDdvnvz8/BQREaG9e/dq4MCBKl68uH788UdXlwbgJsAcIgA3hPT0dA0dOlQJCQkqVaqUoqKiNG7cOFeXBeAmwRkiAABgeUyqBgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlvf/ACyemBwUMF5mAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = traindf['tokens'].apply(len)\n",
    "\n",
    "plt.hist(lengths, bins=10)\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.xlabel('Length of Text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate prediction - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = traindf['trending']\n",
    "y_val = valdf['trending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack(traindf['vector'])\n",
    "X_test = np.stack(valdf['vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4084, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training w/ np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['categoryId', 'num_sub', 'trending', 'lang', 'log_subs', 'tokens',\n",
       "       'vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "[[364 129]\n",
      " [ 62 449]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.74      0.79       493\n",
      "           1       0.78      0.88      0.82       511\n",
      "\n",
      "    accuracy                           0.81      1004\n",
      "   macro avg       0.82      0.81      0.81      1004\n",
      "weighted avg       0.81      0.81      0.81      1004\n",
      "\n",
      "Accuracy: 0.8097609561752988\n"
     ]
    }
   ],
   "source": [
    "# Train SVM\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print(\"SVM:\")\n",
    "print(confusion_matrix(y_val, svm_pred))\n",
    "print(classification_report(y_val, svm_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00355803 0.00543375 0.00319439 0.00254509 0.00365634 0.00244131\n",
      " 0.0090236  0.00271126 0.00223186 0.00263739 0.00291535 0.00206031\n",
      " 0.00220198 0.00322662 0.00691444 0.00261035 0.0109483  0.00253622\n",
      " 0.00195049 0.00296411 0.00260842 0.00526372 0.00289789 0.00312632\n",
      " 0.0027693  0.00252654 0.00224129 0.00253898 0.00351668 0.0025837\n",
      " 0.00336795 0.00277923 0.00252329 0.00968639 0.00308218 0.00257726\n",
      " 0.0037974  0.0030189  0.00537861 0.00247077 0.00246176 0.00321451\n",
      " 0.00220243 0.0021612  0.00508909 0.00310649 0.00246513 0.00255369\n",
      " 0.0036425  0.00251899 0.00370032 0.00236715 0.00263932 0.00295895\n",
      " 0.00232493 0.00278767 0.0026882  0.00253686 0.00412155 0.00320886\n",
      " 0.00247514 0.00254455 0.00288929 0.00315362 0.00458111 0.00697373\n",
      " 0.0039779  0.00217482 0.00272646 0.00303932 0.00365375 0.00260584\n",
      " 0.00327206 0.00206267 0.00192779 0.00359557 0.00435403 0.00216469\n",
      " 0.00256208 0.00194505 0.00732891 0.00574113 0.00256751 0.00354073\n",
      " 0.00236621 0.00508629 0.00315764 0.001687   0.00414647 0.00199974\n",
      " 0.00231457 0.00281188 0.0038147  0.00228688 0.00279127 0.00324323\n",
      " 0.00204501 0.0025919  0.00368432 0.00242286 0.00190737 0.00269215\n",
      " 0.00232753 0.00231921 0.00316609 0.00189863 0.00248076 0.00222876\n",
      " 0.00305575 0.00236    0.00580915 0.00340257 0.00256156 0.00409208\n",
      " 0.00188494 0.00367838 0.00272238 0.00276366 0.00370122 0.00208417\n",
      " 0.00235755 0.00399842 0.00256113 0.00233848 0.0038972  0.00603719\n",
      " 0.00242999 0.00373582 0.00221165 0.00338921 0.00237108 0.00267548\n",
      " 0.00232391 0.00264292 0.00355159 0.00221514 0.00455316 0.00355094\n",
      " 0.0025975  0.00206635 0.0029864  0.01059429 0.00246204 0.00240602\n",
      " 0.00271748 0.00204018 0.00256432 0.00305988 0.00226171 0.00337852\n",
      " 0.00508306 0.00257531 0.00207245 0.00332367 0.00217323 0.00761808\n",
      " 0.00292008 0.00235853 0.00217593 0.00260665 0.00256206 0.00228277\n",
      " 0.00602601 0.00292279 0.0022866  0.00260259 0.00236105 0.00203797\n",
      " 0.00364686 0.00429406 0.00234006 0.00293114 0.00304595 0.00247022\n",
      " 0.002853   0.00330247 0.00229918 0.00255471 0.00275348 0.00272854\n",
      " 0.0019336  0.00274766 0.0169696  0.00289722 0.00251735 0.00264715\n",
      " 0.00269891 0.00237588 0.00460654 0.00339943 0.00417942 0.00265721\n",
      " 0.00299846 0.00389112 0.00301147 0.00246068 0.00214168 0.00296753\n",
      " 0.00255386 0.00244277 0.00320618 0.006335   0.00252811 0.00340873\n",
      " 0.00283903 0.00304272 0.00271212 0.00512169 0.00259071 0.00270058\n",
      " 0.00686097 0.00305435 0.00228891 0.00287271 0.01045938 0.00319697\n",
      " 0.00241965 0.0118024  0.00232546 0.00324515 0.00916152 0.00282184\n",
      " 0.00264535 0.00366929 0.00207172 0.00241236 0.00434903 0.00573284\n",
      " 0.00251946 0.00687567 0.00339119 0.00339568 0.0024301  0.00316432\n",
      " 0.00261087 0.00275512 0.00189971 0.00491601 0.00252232 0.00209776\n",
      " 0.00221683 0.00245935 0.0031036  0.0021309  0.00253481 0.00253813\n",
      " 0.00301501 0.0040893  0.00297895 0.0027556  0.00246031 0.00325045\n",
      " 0.00182122 0.00225452 0.00244034 0.00357988 0.00377056 0.00237532\n",
      " 0.00259509 0.00270979 0.00254683 0.00281513 0.00225408 0.00305789\n",
      " 0.00275312 0.00266108 0.00250657 0.00828246 0.00236206 0.00250844\n",
      " 0.00249018 0.00297614 0.00231    0.00242293 0.00219097 0.00248907\n",
      " 0.00295786 0.00215167 0.0024289  0.00250438 0.00262327 0.00343941\n",
      " 0.00251269 0.00332385 0.00737276 0.00385855 0.00241815 0.00234634\n",
      " 0.00454655 0.00740316 0.00796678 0.00316764 0.00256097 0.00230236\n",
      " 0.00212223 0.0050586  0.00248733 0.01527932 0.00242625 0.00258992]\n",
      "Random Forest:\n",
      "[[368 125]\n",
      " [ 95 416]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77       493\n",
      "           1       0.77      0.81      0.79       511\n",
      "\n",
      "    accuracy                           0.78      1004\n",
      "   macro avg       0.78      0.78      0.78      1004\n",
      "weighted avg       0.78      0.78      0.78      1004\n",
      "\n",
      "Accuracy: 0.7808764940239044\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(rf_model.feature_importances_)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(confusion_matrix(y_val, rf_pred))\n",
    "print(classification_report(y_val, rf_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "[[348 145]\n",
      " [ 82 429]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.75       493\n",
      "           1       0.75      0.84      0.79       511\n",
      "\n",
      "    accuracy                           0.77      1004\n",
      "   macro avg       0.78      0.77      0.77      1004\n",
      "weighted avg       0.78      0.77      0.77      1004\n",
      "\n",
      "Accuracy: 0.7739043824701195\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(confusion_matrix(y_val, lr_pred))\n",
    "print(classification_report(y_val, lr_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, lr_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "       # x = torch.sigmoid(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(X_train, dtype=torch.float)\n",
    "train_labels = torch.tensor(y_train, dtype=torch.long)\n",
    "val_data = torch.tensor(X_test, dtype=torch.float)\n",
    "val_labels = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6056628671940416\n",
      "Epoch 1, Validation Loss: 0.49025419633835554, Validation Accuracy: 0.7729083665338645\n",
      "Epoch 2, Train Loss: 0.5142706767655909\n",
      "Epoch 2, Validation Loss: 0.46821191534399986, Validation Accuracy: 0.7858565737051793\n",
      "Epoch 3, Train Loss: 0.47563710855320096\n",
      "Epoch 3, Validation Loss: 0.4596212236210704, Validation Accuracy: 0.796812749003984\n",
      "Epoch 4, Train Loss: 0.4550440111197531\n",
      "Epoch 4, Validation Loss: 0.4551769308745861, Validation Accuracy: 0.8117529880478087\n",
      "Epoch 5, Train Loss: 0.45030693570151925\n",
      "Epoch 5, Validation Loss: 0.44979717675596476, Validation Accuracy: 0.8047808764940239\n",
      "Epoch 6, Train Loss: 0.4342188936425373\n",
      "Epoch 6, Validation Loss: 0.47628355026245117, Validation Accuracy: 0.7798804780876494\n",
      "Epoch 7, Train Loss: 0.43260300694964826\n",
      "Epoch 7, Validation Loss: 0.4632433708757162, Validation Accuracy: 0.796812749003984\n",
      "Epoch 8, Train Loss: 0.4298697040649131\n",
      "Epoch 8, Validation Loss: 0.47927119163796306, Validation Accuracy: 0.8007968127490039\n",
      "Epoch 9, Train Loss: 0.4252598127350211\n",
      "Epoch 9, Validation Loss: 0.46393943671137094, Validation Accuracy: 0.795816733067729\n",
      "Epoch 10, Train Loss: 0.4079693804960698\n",
      "Epoch 10, Validation Loss: 0.46831674547865987, Validation Accuracy: 0.8077689243027888\n",
      "Epoch 11, Train Loss: 0.4076032049488276\n",
      "Epoch 11, Validation Loss: 0.4641207391396165, Validation Accuracy: 0.8017928286852589\n",
      "Epoch 12, Train Loss: 0.40264681237749755\n",
      "Epoch 12, Validation Loss: 0.4562119971960783, Validation Accuracy: 0.8037848605577689\n",
      "Epoch 13, Train Loss: 0.39052183309104294\n",
      "Epoch 13, Validation Loss: 0.48888120520859957, Validation Accuracy: 0.7838645418326693\n",
      "Epoch 14, Train Loss: 0.38133904174901545\n",
      "Epoch 14, Validation Loss: 0.4757478372193873, Validation Accuracy: 0.8057768924302788\n",
      "Epoch 15, Train Loss: 0.3756122363265604\n",
      "Epoch 15, Validation Loss: 0.4902004888281226, Validation Accuracy: 0.8067729083665338\n",
      "Epoch 16, Train Loss: 0.37166636146139354\n",
      "Epoch 16, Validation Loss: 0.5176329435780644, Validation Accuracy: 0.7928286852589641\n",
      "Epoch 17, Train Loss: 0.3766966589028016\n",
      "Epoch 17, Validation Loss: 0.4833617117255926, Validation Accuracy: 0.7948207171314741\n",
      "Epoch 18, Train Loss: 0.3507526839384809\n",
      "Epoch 18, Validation Loss: 0.47933518700301647, Validation Accuracy: 0.8007968127490039\n",
      "Epoch 19, Train Loss: 0.3593208519741893\n",
      "Epoch 19, Validation Loss: 0.47610829723998904, Validation Accuracy: 0.8007968127490039\n",
      "Epoch 20, Train Loss: 0.34954801737330854\n",
      "Epoch 20, Validation Loss: 0.48039373150095344, Validation Accuracy: 0.7948207171314741\n"
     ]
    }
   ],
   "source": [
    "input_size = 300\n",
    "num_classes = 2\n",
    "#model = SimpleNN(input_size, num_classes)\n",
    "model = AdvancedNN(input_size, num_classes)\n",
    "\n",
    "learning_rate = 0.001\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_dataloader)\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with df\n",
    "\n",
    "Pretty much using some other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf_train = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>-0.010999</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>-0.008287</td>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>-0.012605</td>\n",
       "      <td>-0.054438</td>\n",
       "      <td>-0.011793</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>-0.040189</td>\n",
       "      <td>-0.019612</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.007417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>-0.043199</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>-0.020090</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>-0.030937</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>-0.007100</td>\n",
       "      <td>-0.046828</td>\n",
       "      <td>0.010370</td>\n",
       "      <td>-0.013127</td>\n",
       "      <td>-0.034672</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>-0.016757</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.016839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.016564</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>-0.026991</td>\n",
       "      <td>-0.025944</td>\n",
       "      <td>-0.042773</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015814</td>\n",
       "      <td>0.007675</td>\n",
       "      <td>-0.038544</td>\n",
       "      <td>0.036608</td>\n",
       "      <td>-0.013988</td>\n",
       "      <td>-0.077762</td>\n",
       "      <td>-0.036571</td>\n",
       "      <td>-0.003778</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>0.016832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014575</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>-0.016263</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>0.014698</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>-0.024183</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>-0.055904</td>\n",
       "      <td>0.013691</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>-0.021446</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>-0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.015717</td>\n",
       "      <td>-0.038114</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.027980</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>-0.033920</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030791</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>-0.059791</td>\n",
       "      <td>-0.014414</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.041190</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.041935</td>\n",
       "      <td>-0.014488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0.001493</td>\n",
       "      <td>-0.006056</td>\n",
       "      <td>-0.006911</td>\n",
       "      <td>-0.004492</td>\n",
       "      <td>-0.012382</td>\n",
       "      <td>-0.018271</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>-0.026307</td>\n",
       "      <td>-0.012953</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>-0.002476</td>\n",
       "      <td>-0.053833</td>\n",
       "      <td>0.027181</td>\n",
       "      <td>-0.010399</td>\n",
       "      <td>-0.015832</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>-0.007146</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.012746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>-0.007499</td>\n",
       "      <td>-0.008978</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>-0.010673</td>\n",
       "      <td>-0.002196</td>\n",
       "      <td>0.014975</td>\n",
       "      <td>-0.008162</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>-0.041622</td>\n",
       "      <td>-0.010662</td>\n",
       "      <td>-0.011063</td>\n",
       "      <td>-0.028395</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>-0.045244</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>-0.028481</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>0.015856</td>\n",
       "      <td>0.023422</td>\n",
       "      <td>-0.019607</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.017187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015455</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>-0.024251</td>\n",
       "      <td>-0.005403</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>-0.032007</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>-0.022578</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>-0.022701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>0.003536</td>\n",
       "      <td>-0.032019</td>\n",
       "      <td>-0.030158</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>-0.038139</td>\n",
       "      <td>-0.043196</td>\n",
       "      <td>-0.026681</td>\n",
       "      <td>-0.053684</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>-0.021900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>-0.097177</td>\n",
       "      <td>-0.022242</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.016268</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>0.017021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>-0.010770</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>-0.030562</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005619</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>-0.037379</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>-0.002610</td>\n",
       "      <td>-0.036301</td>\n",
       "      <td>-0.041805</td>\n",
       "      <td>-0.009754</td>\n",
       "      <td>0.009544</td>\n",
       "      <td>-0.008950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4084 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.037324  0.001781 -0.010999 -0.014390  0.008327  0.015050  0.002750   \n",
       "1     0.027126  0.018631 -0.043199  0.010921  0.018408 -0.020090 -0.002293   \n",
       "2    -0.009290  0.001580  0.041077  0.008362 -0.016564  0.003744 -0.026991   \n",
       "3     0.014575 -0.001549 -0.016263  0.012496  0.005753  0.014698  0.013166   \n",
       "4    -0.015717 -0.038114 -0.004080  0.052129  0.016092  0.027980  0.015357   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4079  0.001493 -0.006056 -0.006911 -0.004492 -0.012382 -0.018271  0.001261   \n",
       "4080  0.010417 -0.007499 -0.008978  0.032899 -0.010673 -0.002196  0.014975   \n",
       "4081  0.001642  0.008861 -0.028481  0.004189 -0.002797  0.015856  0.023422   \n",
       "4082  0.003536 -0.032019 -0.030158  0.042114 -0.038139 -0.043196 -0.026681   \n",
       "4083  0.001289  0.012660  0.010781  0.009070 -0.010770 -0.001015  0.012181   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0    -0.008287  0.029170  0.022423  ...  0.003419 -0.012605 -0.054438   \n",
       "1    -0.030937  0.034213  0.029412  ...  0.007731 -0.007100 -0.046828   \n",
       "2    -0.025944 -0.042773  0.002404  ...  0.015814  0.007675 -0.038544   \n",
       "3    -0.024183  0.035358 -0.003001  ...  0.006662  0.015656 -0.055904   \n",
       "4    -0.033920  0.002469  0.001617  ... -0.030791  0.005275 -0.059791   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4079 -0.026307 -0.012953  0.005960  ...  0.003202 -0.002476 -0.053833   \n",
       "4080 -0.008162  0.000760 -0.001148  ...  0.004064  0.029870 -0.041622   \n",
       "4081 -0.019607  0.038859  0.017187  ... -0.015455 -0.004521 -0.024251   \n",
       "4082 -0.053684  0.019157 -0.021900  ...  0.001806  0.008763 -0.097177   \n",
       "4083 -0.030562  0.009457  0.030358  ... -0.005619  0.003087 -0.037379   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0    -0.011793  0.005213 -0.040189 -0.019612 -0.008356 -0.002161  0.007417  \n",
       "1     0.010370 -0.013127 -0.034672  0.009649 -0.016757  0.003185  0.016839  \n",
       "2     0.036608 -0.013988 -0.077762 -0.036571 -0.003778 -0.001089  0.016832  \n",
       "3     0.013691  0.000369 -0.021446  0.012411 -0.036735  0.013220 -0.000223  \n",
       "4    -0.014414  0.000138 -0.041190  0.003861 -0.003530 -0.041935 -0.014488  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4079  0.027181 -0.010399 -0.015832  0.001375 -0.007146  0.016459  0.012746  \n",
       "4080 -0.010662 -0.011063 -0.028395  0.003778 -0.045244  0.007093  0.001836  \n",
       "4081 -0.005403  0.007940 -0.032007  0.002156 -0.022578  0.020213 -0.022701  \n",
       "4082 -0.022242  0.010019  0.001093  0.010236  0.016268 -0.007597  0.017021  \n",
       "4083  0.001822 -0.002610 -0.036301 -0.041805 -0.009754  0.009544 -0.008950  \n",
       "\n",
       "[4084 rows x 300 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cate_0', 'cate_1', 'cate_2', 'cate_3', 'cate_4', 'num_sub', 'trending',\n",
       "       'lang', 'log_subs', 'tokens', 'vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdf = traindf[['cate_0', 'cate_1', 'cate_2', 'cate_3', 'cate_4', 'log_subs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryId</th>\n",
       "      <th>num_sub</th>\n",
       "      <th>trending</th>\n",
       "      <th>lang</th>\n",
       "      <th>log_subs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>754000</td>\n",
       "      <td>1</td>\n",
       "      <td>Power Book II: Ghost | Official Trailer | Seas...</td>\n",
       "      <td>13.533149</td>\n",
       "      <td>[Power, Book, II, ghost, official, Trailer, Se...</td>\n",
       "      <td>[0.03732416, 0.0017805065, -0.010998716, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>314000</td>\n",
       "      <td>1</td>\n",
       "      <td>Julian Newman WENT OFF Against Tristan Jass an...</td>\n",
       "      <td>12.657151</td>\n",
       "      <td>[Julian, Newman, go, Tristan, Jass, Carson, Ro...</td>\n",
       "      <td>[0.027126025, 0.018631326, -0.04319948, 0.0109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>1290000</td>\n",
       "      <td>1</td>\n",
       "      <td>GET READY WITH US TO GO TO LES DO MAKEUP'S AND...</td>\n",
       "      <td>14.070154</td>\n",
       "      <td>[ready, LES, MAKEUP, KB, EVENT, Murillo, SOCIA...</td>\n",
       "      <td>[-0.0092903115, 0.0015797918, 0.041077197, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>10000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Binging with Babish: Tater Tots from Breaking ...</td>\n",
       "      <td>16.118096</td>\n",
       "      <td>[binge, Babish, Tater, Tots, break, bad, episo...</td>\n",
       "      <td>[0.014575143, -0.0015489127, -0.016263451, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>145000</td>\n",
       "      <td>0</td>\n",
       "      <td>My Friend sister Birthday Cake doll cake kaise...</td>\n",
       "      <td>11.884496</td>\n",
       "      <td>[friend, sister, Birthday, Cake, doll, cake, k...</td>\n",
       "      <td>[-0.015716612, -0.038114376, -0.0040804143, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>10</td>\n",
       "      <td>7960</td>\n",
       "      <td>1</td>\n",
       "      <td>Aaron Mercury - Apaga La Luz Aaron|Mercury|Apa...</td>\n",
       "      <td>8.982310</td>\n",
       "      <td>[Aaron, Mercury, La, Luz, music, video, Aaron,...</td>\n",
       "      <td>[0.0014932244, -0.006056428, -0.0069105057, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>20</td>\n",
       "      <td>934000</td>\n",
       "      <td>0</td>\n",
       "      <td>Sonic And Amy Funny Dance - Sonic And Amy Stor...</td>\n",
       "      <td>13.747233</td>\n",
       "      <td>[sonic, Amy, Funny, dance, Sonic, Amy, Story, ...</td>\n",
       "      <td>[0.010417218, -0.007498604, -0.008977798, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>2</td>\n",
       "      <td>12800</td>\n",
       "      <td>0</td>\n",
       "      <td>Check This Out!  Custom Skoolie Exterior Tour!...</td>\n",
       "      <td>9.457279</td>\n",
       "      <td>[check, Custom, Exterior, Tour, travel, tour, ...</td>\n",
       "      <td>[0.0016421076, 0.008860779, -0.028481334, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>22</td>\n",
       "      <td>33100</td>\n",
       "      <td>0</td>\n",
       "      <td>Viral aiya susanti #shorts drako|drama lucu|fu...</td>\n",
       "      <td>10.407319</td>\n",
       "      <td>[viral, short, Drama]</td>\n",
       "      <td>[0.0035358805, -0.032018986, -0.030157546, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>24</td>\n",
       "      <td>18700000</td>\n",
       "      <td>0</td>\n",
       "      <td>Khooni Khel | CID (Bengali) - Ep 1307 | Full E...</td>\n",
       "      <td>16.744034</td>\n",
       "      <td>[Khooni, Khel, CID, Bengali, Ep, episode, Mar,...</td>\n",
       "      <td>[0.0012892334, 0.012660025, 0.010780661, 0.009...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4084 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      categoryId   num_sub  trending  \\\n",
       "0             24    754000         1   \n",
       "1             17    314000         1   \n",
       "2             22   1290000         1   \n",
       "3             22  10000000         1   \n",
       "4             26    145000         0   \n",
       "...          ...       ...       ...   \n",
       "4079          10      7960         1   \n",
       "4080          20    934000         0   \n",
       "4081           2     12800         0   \n",
       "4082          22     33100         0   \n",
       "4083          24  18700000         0   \n",
       "\n",
       "                                                   lang   log_subs  \\\n",
       "0     Power Book II: Ghost | Official Trailer | Seas...  13.533149   \n",
       "1     Julian Newman WENT OFF Against Tristan Jass an...  12.657151   \n",
       "2     GET READY WITH US TO GO TO LES DO MAKEUP'S AND...  14.070154   \n",
       "3     Binging with Babish: Tater Tots from Breaking ...  16.118096   \n",
       "4     My Friend sister Birthday Cake doll cake kaise...  11.884496   \n",
       "...                                                 ...        ...   \n",
       "4079  Aaron Mercury - Apaga La Luz Aaron|Mercury|Apa...   8.982310   \n",
       "4080  Sonic And Amy Funny Dance - Sonic And Amy Stor...  13.747233   \n",
       "4081  Check This Out!  Custom Skoolie Exterior Tour!...   9.457279   \n",
       "4082  Viral aiya susanti #shorts drako|drama lucu|fu...  10.407319   \n",
       "4083  Khooni Khel | CID (Bengali) - Ep 1307 | Full E...  16.744034   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [Power, Book, II, ghost, official, Trailer, Se...   \n",
       "1     [Julian, Newman, go, Tristan, Jass, Carson, Ro...   \n",
       "2     [ready, LES, MAKEUP, KB, EVENT, Murillo, SOCIA...   \n",
       "3     [binge, Babish, Tater, Tots, break, bad, episo...   \n",
       "4     [friend, sister, Birthday, Cake, doll, cake, k...   \n",
       "...                                                 ...   \n",
       "4079  [Aaron, Mercury, La, Luz, music, video, Aaron,...   \n",
       "4080  [sonic, Amy, Funny, dance, Sonic, Amy, Story, ...   \n",
       "4081  [check, Custom, Exterior, Tour, travel, tour, ...   \n",
       "4082                              [viral, short, Drama]   \n",
       "4083  [Khooni, Khel, CID, Bengali, Ep, episode, Mar,...   \n",
       "\n",
       "                                                 vector  \n",
       "0     [0.03732416, 0.0017805065, -0.010998716, -0.01...  \n",
       "1     [0.027126025, 0.018631326, -0.04319948, 0.0109...  \n",
       "2     [-0.0092903115, 0.0015797918, 0.041077197, 0.0...  \n",
       "3     [0.014575143, -0.0015489127, -0.016263451, 0.0...  \n",
       "4     [-0.015716612, -0.038114376, -0.0040804143, 0....  \n",
       "...                                                 ...  \n",
       "4079  [0.0014932244, -0.006056428, -0.0069105057, -0...  \n",
       "4080  [0.010417218, -0.007498604, -0.008977798, 0.03...  \n",
       "4081  [0.0016421076, 0.008860779, -0.028481334, 0.00...  \n",
       "4082  [0.0035358805, -0.032018986, -0.030157546, 0.0...  \n",
       "4083  [0.0012892334, 0.012660025, 0.010780661, 0.009...  \n",
       "\n",
       "[4084 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cate_0</th>\n",
       "      <th>cate_1</th>\n",
       "      <th>cate_2</th>\n",
       "      <th>cate_3</th>\n",
       "      <th>cate_4</th>\n",
       "      <th>log_subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.533149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.657151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.070154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.118096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.884496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.982310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.747233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.457279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.407319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.744034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4084 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cate_0  cate_1  cate_2  cate_3  cate_4   log_subs\n",
       "0          0       0       0       1       1  13.533149\n",
       "1          1       0       0       0       1  12.657151\n",
       "2          0       1       1       0       1  14.070154\n",
       "3          0       1       1       0       1  16.118096\n",
       "4          0       1       0       1       1  11.884496\n",
       "...      ...     ...     ...     ...     ...        ...\n",
       "4079       0       1       0       1       0   8.982310\n",
       "4080       0       0       1       0       1  13.747233\n",
       "4081       0       1       0       0       0   9.457279\n",
       "4082       0       1       1       0       1  10.407319\n",
       "4083       0       0       0       1       1  16.744034\n",
       "\n",
       "[4084 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedf = valdf[['cate_0', 'cate_1', 'cate_2', 'cate_3', 'cate_4', 'log_subs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTrain = pd.concat([Xdf_train, trdf], axis=1, sort=False)\n",
    "resultTest = pd.concat([Xdf_test, tedf], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>cate_0</th>\n",
       "      <th>cate_1</th>\n",
       "      <th>cate_2</th>\n",
       "      <th>cate_3</th>\n",
       "      <th>cate_4</th>\n",
       "      <th>log_subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>-0.010999</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>-0.008287</td>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019612</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.533149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>-0.043199</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>-0.020090</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>-0.030937</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>-0.016757</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.657151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.016564</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>-0.026991</td>\n",
       "      <td>-0.025944</td>\n",
       "      <td>-0.042773</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036571</td>\n",
       "      <td>-0.003778</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>0.016832</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.070154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014575</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>-0.016263</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>0.014698</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>-0.024183</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.118096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.015717</td>\n",
       "      <td>-0.038114</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.027980</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>-0.033920</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.041935</td>\n",
       "      <td>-0.014488</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.884496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0.001493</td>\n",
       "      <td>-0.006056</td>\n",
       "      <td>-0.006911</td>\n",
       "      <td>-0.004492</td>\n",
       "      <td>-0.012382</td>\n",
       "      <td>-0.018271</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>-0.026307</td>\n",
       "      <td>-0.012953</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>-0.007146</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.012746</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.982310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>-0.007499</td>\n",
       "      <td>-0.008978</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>-0.010673</td>\n",
       "      <td>-0.002196</td>\n",
       "      <td>0.014975</td>\n",
       "      <td>-0.008162</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>-0.045244</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.747233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>-0.028481</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>0.015856</td>\n",
       "      <td>0.023422</td>\n",
       "      <td>-0.019607</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.017187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>-0.022578</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>-0.022701</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.457279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>0.003536</td>\n",
       "      <td>-0.032019</td>\n",
       "      <td>-0.030158</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>-0.038139</td>\n",
       "      <td>-0.043196</td>\n",
       "      <td>-0.026681</td>\n",
       "      <td>-0.053684</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>-0.021900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.016268</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.407319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>-0.010770</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>-0.030562</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041805</td>\n",
       "      <td>-0.009754</td>\n",
       "      <td>0.009544</td>\n",
       "      <td>-0.008950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.744034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4084 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.037324  0.001781 -0.010999 -0.014390  0.008327  0.015050  0.002750   \n",
       "1     0.027126  0.018631 -0.043199  0.010921  0.018408 -0.020090 -0.002293   \n",
       "2    -0.009290  0.001580  0.041077  0.008362 -0.016564  0.003744 -0.026991   \n",
       "3     0.014575 -0.001549 -0.016263  0.012496  0.005753  0.014698  0.013166   \n",
       "4    -0.015717 -0.038114 -0.004080  0.052129  0.016092  0.027980  0.015357   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4079  0.001493 -0.006056 -0.006911 -0.004492 -0.012382 -0.018271  0.001261   \n",
       "4080  0.010417 -0.007499 -0.008978  0.032899 -0.010673 -0.002196  0.014975   \n",
       "4081  0.001642  0.008861 -0.028481  0.004189 -0.002797  0.015856  0.023422   \n",
       "4082  0.003536 -0.032019 -0.030158  0.042114 -0.038139 -0.043196 -0.026681   \n",
       "4083  0.001289  0.012660  0.010781  0.009070 -0.010770 -0.001015  0.012181   \n",
       "\n",
       "             7         8         9  ...       296       297       298  \\\n",
       "0    -0.008287  0.029170  0.022423  ... -0.019612 -0.008356 -0.002161   \n",
       "1    -0.030937  0.034213  0.029412  ...  0.009649 -0.016757  0.003185   \n",
       "2    -0.025944 -0.042773  0.002404  ... -0.036571 -0.003778 -0.001089   \n",
       "3    -0.024183  0.035358 -0.003001  ...  0.012411 -0.036735  0.013220   \n",
       "4    -0.033920  0.002469  0.001617  ...  0.003861 -0.003530 -0.041935   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4079 -0.026307 -0.012953  0.005960  ...  0.001375 -0.007146  0.016459   \n",
       "4080 -0.008162  0.000760 -0.001148  ...  0.003778 -0.045244  0.007093   \n",
       "4081 -0.019607  0.038859  0.017187  ...  0.002156 -0.022578  0.020213   \n",
       "4082 -0.053684  0.019157 -0.021900  ...  0.010236  0.016268 -0.007597   \n",
       "4083 -0.030562  0.009457  0.030358  ... -0.041805 -0.009754  0.009544   \n",
       "\n",
       "           299  cate_0  cate_1  cate_2  cate_3  cate_4   log_subs  \n",
       "0     0.007417       0       0       0       1       1  13.533149  \n",
       "1     0.016839       1       0       0       0       1  12.657151  \n",
       "2     0.016832       0       1       1       0       1  14.070154  \n",
       "3    -0.000223       0       1       1       0       1  16.118096  \n",
       "4    -0.014488       0       1       0       1       1  11.884496  \n",
       "...        ...     ...     ...     ...     ...     ...        ...  \n",
       "4079  0.012746       0       1       0       1       0   8.982310  \n",
       "4080  0.001836       0       0       1       0       1  13.747233  \n",
       "4081 -0.022701       0       1       0       0       0   9.457279  \n",
       "4082  0.017021       0       1       1       0       1  10.407319  \n",
       "4083 -0.008950       0       0       0       1       1  16.744034  \n",
       "\n",
       "[4084 rows x 306 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTrain.columns = resultTrain.columns.astype(str)\n",
    "resultTest.columns = resultTest.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>cate_0</th>\n",
       "      <th>cate_1</th>\n",
       "      <th>cate_2</th>\n",
       "      <th>cate_3</th>\n",
       "      <th>cate_4</th>\n",
       "      <th>log_subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>-0.010999</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>-0.008287</td>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019612</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.533149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>-0.043199</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>-0.020090</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>-0.030937</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>-0.016757</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.657151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.016564</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>-0.026991</td>\n",
       "      <td>-0.025944</td>\n",
       "      <td>-0.042773</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036571</td>\n",
       "      <td>-0.003778</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>0.016832</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.070154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014575</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>-0.016263</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>0.014698</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>-0.024183</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.118096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.015717</td>\n",
       "      <td>-0.038114</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.027980</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>-0.033920</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.041935</td>\n",
       "      <td>-0.014488</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.884496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0.001493</td>\n",
       "      <td>-0.006056</td>\n",
       "      <td>-0.006911</td>\n",
       "      <td>-0.004492</td>\n",
       "      <td>-0.012382</td>\n",
       "      <td>-0.018271</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>-0.026307</td>\n",
       "      <td>-0.012953</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>-0.007146</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.012746</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.982310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>-0.007499</td>\n",
       "      <td>-0.008978</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>-0.010673</td>\n",
       "      <td>-0.002196</td>\n",
       "      <td>0.014975</td>\n",
       "      <td>-0.008162</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>-0.045244</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.747233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>-0.028481</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>0.015856</td>\n",
       "      <td>0.023422</td>\n",
       "      <td>-0.019607</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.017187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>-0.022578</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>-0.022701</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.457279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>0.003536</td>\n",
       "      <td>-0.032019</td>\n",
       "      <td>-0.030158</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>-0.038139</td>\n",
       "      <td>-0.043196</td>\n",
       "      <td>-0.026681</td>\n",
       "      <td>-0.053684</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>-0.021900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.016268</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.407319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>-0.010770</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>-0.030562</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041805</td>\n",
       "      <td>-0.009754</td>\n",
       "      <td>0.009544</td>\n",
       "      <td>-0.008950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.744034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4084 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.037324  0.001781 -0.010999 -0.014390  0.008327  0.015050  0.002750   \n",
       "1     0.027126  0.018631 -0.043199  0.010921  0.018408 -0.020090 -0.002293   \n",
       "2    -0.009290  0.001580  0.041077  0.008362 -0.016564  0.003744 -0.026991   \n",
       "3     0.014575 -0.001549 -0.016263  0.012496  0.005753  0.014698  0.013166   \n",
       "4    -0.015717 -0.038114 -0.004080  0.052129  0.016092  0.027980  0.015357   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4079  0.001493 -0.006056 -0.006911 -0.004492 -0.012382 -0.018271  0.001261   \n",
       "4080  0.010417 -0.007499 -0.008978  0.032899 -0.010673 -0.002196  0.014975   \n",
       "4081  0.001642  0.008861 -0.028481  0.004189 -0.002797  0.015856  0.023422   \n",
       "4082  0.003536 -0.032019 -0.030158  0.042114 -0.038139 -0.043196 -0.026681   \n",
       "4083  0.001289  0.012660  0.010781  0.009070 -0.010770 -0.001015  0.012181   \n",
       "\n",
       "             7         8         9  ...       296       297       298  \\\n",
       "0    -0.008287  0.029170  0.022423  ... -0.019612 -0.008356 -0.002161   \n",
       "1    -0.030937  0.034213  0.029412  ...  0.009649 -0.016757  0.003185   \n",
       "2    -0.025944 -0.042773  0.002404  ... -0.036571 -0.003778 -0.001089   \n",
       "3    -0.024183  0.035358 -0.003001  ...  0.012411 -0.036735  0.013220   \n",
       "4    -0.033920  0.002469  0.001617  ...  0.003861 -0.003530 -0.041935   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4079 -0.026307 -0.012953  0.005960  ...  0.001375 -0.007146  0.016459   \n",
       "4080 -0.008162  0.000760 -0.001148  ...  0.003778 -0.045244  0.007093   \n",
       "4081 -0.019607  0.038859  0.017187  ...  0.002156 -0.022578  0.020213   \n",
       "4082 -0.053684  0.019157 -0.021900  ...  0.010236  0.016268 -0.007597   \n",
       "4083 -0.030562  0.009457  0.030358  ... -0.041805 -0.009754  0.009544   \n",
       "\n",
       "           299  cate_0  cate_1  cate_2  cate_3  cate_4   log_subs  \n",
       "0     0.007417       0       0       0       1       1  13.533149  \n",
       "1     0.016839       1       0       0       0       1  12.657151  \n",
       "2     0.016832       0       1       1       0       1  14.070154  \n",
       "3    -0.000223       0       1       1       0       1  16.118096  \n",
       "4    -0.014488       0       1       0       1       1  11.884496  \n",
       "...        ...     ...     ...     ...     ...     ...        ...  \n",
       "4079  0.012746       0       1       0       1       0   8.982310  \n",
       "4080  0.001836       0       0       1       0       1  13.747233  \n",
       "4081 -0.022701       0       1       0       0       0   9.457279  \n",
       "4082  0.017021       0       1       1       0       1  10.407319  \n",
       "4083 -0.008950       0       0       0       1       1  16.744034  \n",
       "\n",
       "[4084 rows x 306 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "[[252 241]\n",
      " [ 92 419]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.51      0.60       493\n",
      "           1       0.63      0.82      0.72       511\n",
      "\n",
      "    accuracy                           0.67      1004\n",
      "   macro avg       0.68      0.67      0.66      1004\n",
      "weighted avg       0.68      0.67      0.66      1004\n",
      "\n",
      "Accuracy: 0.6683266932270916\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC()\n",
    "svm_model.fit(resultTrain, y_train)\n",
    "svm_pred = svm_model.predict(resultTest)\n",
    "\n",
    "print(\"SVM:\")\n",
    "print(confusion_matrix(y_val, svm_pred))\n",
    "print(classification_report(y_val, svm_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4084, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score 0.531097\n",
      "Root mean squared error: 0.3943196017275116\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = ((y_pred - y_val) ** 2).mean()\n",
    "rmse = mse ** 0.5\n",
    "print('Root mean squared error:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76763\n",
      "[LightGBM] [Info] Number of data points in the train set: 4084, number of used features: 306\n",
      "[LightGBM] [Info] Start training from score 0.531097\n",
      "Root mean squared error: 0.37775378077938404\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(resultTrain, label=y_train)\n",
    "\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "y_pred = model.predict(resultTest)\n",
    "\n",
    "mse = ((y_pred - y_val) ** 2).mean()\n",
    "rmse = mse ** 0.5\n",
    "print('Root mean squared error:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X.iloc[idx].values, dtype=torch.float), torch.tensor(self.y.iloc[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = resultTrain.shape[1]\n",
    "\n",
    "train_dataset = CustomDataset(resultTrain, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(resultTest, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6903237160295248\n",
      "Epoch 1, Validation Loss: 0.6920876186341047, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 2, Train Loss: 0.6882166746072471\n",
      "Epoch 2, Validation Loss: 0.6894653402268887, Validation Accuracy: 0.5109561752988048\n",
      "Epoch 3, Train Loss: 0.6819381746463478\n",
      "Epoch 3, Validation Loss: 0.6905853264033794, Validation Accuracy: 0.5268924302788844\n",
      "Epoch 4, Train Loss: 0.6688341968692839\n",
      "Epoch 4, Validation Loss: 0.6697787996381521, Validation Accuracy: 0.5946215139442231\n",
      "Epoch 5, Train Loss: 0.675971454475075\n",
      "Epoch 5, Validation Loss: 0.6726722903549671, Validation Accuracy: 0.5966135458167331\n",
      "Epoch 6, Train Loss: 0.674051676876843\n",
      "Epoch 6, Validation Loss: 0.6765123140066862, Validation Accuracy: 0.5826693227091634\n",
      "Epoch 7, Train Loss: 0.6702292459085584\n",
      "Epoch 7, Validation Loss: 0.6861137095838785, Validation Accuracy: 0.5517928286852589\n",
      "Epoch 8, Train Loss: 0.6876040799543262\n",
      "Epoch 8, Validation Loss: 0.6987478081136942, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 9, Train Loss: 0.6938432576134801\n",
      "Epoch 9, Validation Loss: 0.6862226109951735, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 10, Train Loss: 0.6856135125271976\n",
      "Epoch 10, Validation Loss: 0.6807598304003477, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 11, Train Loss: 0.6917736502364278\n",
      "Epoch 11, Validation Loss: 0.6913543418049812, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 12, Train Loss: 0.693077455740422\n",
      "Epoch 12, Validation Loss: 0.6900245472788811, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 13, Train Loss: 0.6900992151349783\n",
      "Epoch 13, Validation Loss: 0.6930537689477205, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 14, Train Loss: 0.6898056045174599\n",
      "Epoch 14, Validation Loss: 0.6815042030066252, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 15, Train Loss: 0.690726145170629\n",
      "Epoch 15, Validation Loss: 0.693143842741847, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 16, Train Loss: 0.6931405933573842\n",
      "Epoch 16, Validation Loss: 0.6931417714804411, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 17, Train Loss: 0.6931369416415691\n",
      "Epoch 17, Validation Loss: 0.6931398436427116, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 18, Train Loss: 0.6931330352090299\n",
      "Epoch 18, Validation Loss: 0.6931378245353699, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 19, Train Loss: 0.693130237981677\n",
      "Epoch 19, Validation Loss: 0.6931385025382042, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 20, Train Loss: 0.693128009326756\n",
      "Epoch 20, Validation Loss: 0.6931389179080725, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 21, Train Loss: 0.6931273113004863\n",
      "Epoch 21, Validation Loss: 0.6931339055299759, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 22, Train Loss: 0.6931258165277541\n",
      "Epoch 22, Validation Loss: 0.6931367330253124, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 23, Train Loss: 0.6931246160529554\n",
      "Epoch 23, Validation Loss: 0.6931362152099609, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 24, Train Loss: 0.6931239096447825\n",
      "Epoch 24, Validation Loss: 0.6931353285908699, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 25, Train Loss: 0.6931223664432764\n",
      "Epoch 25, Validation Loss: 0.6931348238140345, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 26, Train Loss: 0.6931214355863631\n",
      "Epoch 26, Validation Loss: 0.6931332219392061, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 27, Train Loss: 0.6931207906454802\n",
      "Epoch 27, Validation Loss: 0.6931362021714449, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 28, Train Loss: 0.6931191673502326\n",
      "Epoch 28, Validation Loss: 0.6931327227503061, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 29, Train Loss: 0.6931181666441262\n",
      "Epoch 29, Validation Loss: 0.6931287832558155, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 30, Train Loss: 0.6923333955928683\n",
      "Epoch 30, Validation Loss: 0.6918808147311211, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 31, Train Loss: 0.6887300573289394\n",
      "Epoch 31, Validation Loss: 0.6894390657544136, Validation Accuracy: 0.5089641434262948\n",
      "Epoch 32, Train Loss: 0.6871105623431504\n",
      "Epoch 32, Validation Loss: 0.6931426227092743, Validation Accuracy: 0.4910358565737052\n",
      "Epoch 33, Train Loss: 0.6910792700946331\n",
      "Epoch 33, Validation Loss: 0.6843896768987179, Validation Accuracy: 0.5338645418326693\n",
      "Epoch 34, Train Loss: 0.6782815516926348\n",
      "Epoch 34, Validation Loss: 0.6815113369375467, Validation Accuracy: 0.5398406374501992\n",
      "Epoch 35, Train Loss: 0.680916287470609\n",
      "Epoch 35, Validation Loss: 0.6931555941700935, Validation Accuracy: 0.4910358565737052\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "\n",
    "num_epochs = 35\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.squeeze())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_dataloader)\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1004.000000\n",
       "mean       14.151494\n",
       "std         2.160089\n",
       "min         6.018593\n",
       "25%        12.936036\n",
       "50%        14.496076\n",
       "75%        15.792363\n",
       "max        19.291974\n",
       "Name: log_subs, dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultTest['log_subs'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a29500abb6bdb3556fdfc455803f9bb3348bbc89ce500f32511904c582090f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
